{"http://go-colly.org/":[{"elementTag":"h2","innerText":"fast and elegant scraping framework for gophers"},{"elementTag":"h2","innerText":"colly provides a clean interface to write any kind of crawler/scraper/spider"},{"elementTag":"p","innerText":"with colly you can easily extract structured data from websites, which can be used for a wide range of applications, like data mining, data processing or archiving."},{"elementTag":"h2","innerText":"features"},{"elementTag":"ul","innerText":"           clean api           fast (\u003e1k request/sec on a single core)           manages request delays and maximum concurrency per domain           automatic cookie and session handling           sync/async/parallel scraping           distributed scraping           caching           automatic encoding of non-unicode responses           robots.txt support           google app engine support         "},{"elementTag":"h2","innerText":"                                      batteries included             "},{"elementTag":"p","innerText":"colly comes with all the tools you need for scraping."},{"elementTag":"h2","innerText":"                                  business ready             "},{"elementTag":"p","innerText":"free for commercial use."},{"elementTag":"h2","innerText":"                                  open source             "},{"elementTag":"p","innerText":"development of colly is community driven and public."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"http://go-colly.org/articles/":[{"elementTag":"h1","innerText":"data scraping articles"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"http://go-colly.org/contact/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"contact us"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"http://go-colly.org/datasets/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"datasets collected by our scrapers"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"http://go-colly.org/docs/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"documentation"},{"elementTag":"p","innerText":"colly is a golang framework for building web scrapers. with colly you can build web scrapers of various complexity, from simple scraper to complex asynchronous website crawlers processing millions of web pages. colly provides an api for performing network requests and for handling the received content (e.g. interacting with dom tree of the html document)."},{"elementTag":"p","innerText":"below you will find some of the most common and helpful pages from our documentation."},{"elementTag":"ul","innerText":" installation guide getting started basic scraper example "},{"elementTag":"p","innerText":"also, check out godoc for api reference."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"http://go-colly.org/docs/examples/basic/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"basic"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"http://go-colly.org/docs/introduction/install/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"how to install"},{"elementTag":"p","innerText":"colly has only one prerequisite and that is golang programming language. you can install it using their installation guide."},{"elementTag":"h3","innerText":"install colly"},{"elementTag":"p","innerText":"type following command on terminal and hit enter to install colly."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"http://go-colly.org/docs/introduction/start/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"getting started"},{"elementTag":"p","innerText":"before working with colly ensure that you have the latest version. see installation guide for more details."},{"elementTag":"p","innerText":"let’s get started with some simple examples."},{"elementTag":"p","innerText":"first, you need to import colly to your codebase:"},{"elementTag":"h2","innerText":"collector"},{"elementTag":"p","innerText":"colly’s main entity is a collector object. collector manages the network communication and responsible for the execution of the attached callbacks while a collector job is running. to work with colly, you have to initialize a collector:"},{"elementTag":"h2","innerText":"callbacks"},{"elementTag":"p","innerText":"you can attach different type of callback functions to a collector to control a collecting job or retrieve information. check out the related section in the package documentation."},{"elementTag":"h3","innerText":"add callbacks to a collector"},{"elementTag":"h3","innerText":"call order of callbacks"},{"elementTag":"h4","innerText":"1. onrequest"},{"elementTag":"p","innerText":"called before a request"},{"elementTag":"h4","innerText":"2. onerror"},{"elementTag":"p","innerText":"called if error occured during the request"},{"elementTag":"h4","innerText":"3. onresponse"},{"elementTag":"p","innerText":"called after response received"},{"elementTag":"h4","innerText":"4. onhtml"},{"elementTag":"p","innerText":"called right after onresponse if the received content is html"},{"elementTag":"h4","innerText":"5. onxml"},{"elementTag":"p","innerText":"called right after onhtml if the received content is html or xml"},{"elementTag":"h4","innerText":"6. onscraped"},{"elementTag":"p","innerText":"called after onxml callbacks"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"http://go-colly.org/services/":[{"elementTag":"h2","innerText":"services"},{"elementTag":"h3","innerText":"custom data collection"},{"elementTag":"p","innerText":"you provide the requirements and our scraping gurus deliver the desired data to you."},{"elementTag":"h3","innerText":"                                  free             "},{"elementTag":"p","innerText":"free data collection service to support ngos and non-profits.we to get data for you."},{"elementTag":"h4","innerText":"only for ngos and non-profits"},{"elementTag":"h3","innerText":"                                  instant             "},{"elementTag":"p","innerText":"gather data from up to 10 websites within a week.fast and reliable."},{"elementTag":"h4","innerText":"$200 per site"},{"elementTag":"h3","innerText":"                                  advanced             "},{"elementTag":"p","innerText":"unlimited number of websites.let us handle your scrapers and enjoy the data."},{"elementTag":"h4","innerText":"from $1,500"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org":[{"elementTag":"h2","innerText":"fast and elegant scraping framework for gophers"},{"elementTag":"h2","innerText":"colly provides a clean interface to write any kind of crawler/scraper/spider"},{"elementTag":"p","innerText":"with colly you can easily extract structured data from websites, which can be used for a wide range of applications, like data mining, data processing or archiving."},{"elementTag":"h2","innerText":"features"},{"elementTag":"ul","innerText":"           clean api           fast (\u003e1k request/sec on a single core)           manages request delays and maximum concurrency per domain           automatic cookie and session handling           sync/async/parallel scraping           distributed scraping           caching           automatic encoding of non-unicode responses           robots.txt support           google app engine support         "},{"elementTag":"h2","innerText":"                                      batteries included             "},{"elementTag":"p","innerText":"colly comes with all the tools you need for scraping."},{"elementTag":"h2","innerText":"                                  business ready             "},{"elementTag":"p","innerText":"free for commercial use."},{"elementTag":"h2","innerText":"                                  open source             "},{"elementTag":"p","innerText":"development of colly is community driven and public."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/":[{"elementTag":"h2","innerText":"fast and elegant scraping framework for gophers"},{"elementTag":"h2","innerText":"colly provides a clean interface to write any kind of crawler/scraper/spider"},{"elementTag":"p","innerText":"with colly you can easily extract structured data from websites, which can be used for a wide range of applications, like data mining, data processing or archiving."},{"elementTag":"h2","innerText":"features"},{"elementTag":"ul","innerText":"           clean api           fast (\u003e1k request/sec on a single core)           manages request delays and maximum concurrency per domain           automatic cookie and session handling           sync/async/parallel scraping           distributed scraping           caching           automatic encoding of non-unicode responses           robots.txt support           google app engine support         "},{"elementTag":"h2","innerText":"                                      batteries included             "},{"elementTag":"p","innerText":"colly comes with all the tools you need for scraping."},{"elementTag":"h2","innerText":"                                  business ready             "},{"elementTag":"p","innerText":"free for commercial use."},{"elementTag":"h2","innerText":"                                  open source             "},{"elementTag":"p","innerText":"development of colly is community driven and public."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/articles/":[{"elementTag":"h1","innerText":"data scraping articles"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/articles/first_release/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"colly goes 1.0"},{"elementTag":"p","innerText":"we are happy to announce that the first major release of colly is here. our goal was to create a scraping framework to speed up development and let its users concentrate on collecting relevant data. there is no need to reinvent the wheel when writing a new collector. scrapers built on top of colly support different storage backends, dynamic configuration and running requests in parallel out of the box. it is also possible to run your scrapers in a distributed manner."},{"elementTag":"h2","innerText":"facts about the development"},{"elementTag":"p","innerText":"it started in september 2017 and has not stopped since. colly has attracted numerous developers who helped by providing valuable feedback and contributing new features. let's see the numbers. in the last seven months 30 contributors have created 338 commits. users have opened 78 issues. 74 of the those were resolved in a few days. contributors have opened 59 pull requests and all of them except for one are either got merged or closed. we would like to thank all of our supporters who either contributed code or wrote blog posts about colly or helped development somehow. we would not be here without you."},{"elementTag":"h2","innerText":"milestones"},{"elementTag":"p","innerText":"// create a dataset (allows two way data-binding) var items = new vis.dataset([ {id: 1, content: ‘first commit’, start: ‘2017-09-29’}, {id: 2, content: ‘first external contribution’, start: ‘2017-10-05’}, {id: 3, content: ‘caching implemented’, start: ‘2017-10-08’}, {id: 4, content: ‘declarative html parsing’, start: ‘2017-11-10’}, {id: 5, content: ‘repository moved to gocolly organization’, start: ‘2017-11-11’}, {id: 6, content: ‘debugger interface added’, start: ‘2017-11-14’}, {id: 7, content: ‘xpath and xml support’, start: ‘2018-01-13’}, {id: 8, content: ‘async crawling’, start: ‘2018-01-20’}, {id: 9, content: ‘storage interface implemented’, start: ‘2018-02-09’}, {id: 10, content: ‘performance optimizations (+25% speed)’, start: ‘2018-03-03’}, {id: 11, content: ‘extensions added’, start: ‘2018-03-12’}, {id: 12, content: ‘twitter account created’, start: ‘2018-03-13’}, {id: 13, content: ‘requests queue interface’, start: ‘2018-04-13’}, {id: 14, content: ‘version 1.0.0’, start: ‘2018-05-13’}, ]);"},{"elementTag":"p","innerText":"// configuration for the timeline var options = { stack: true, tooltip: { //       followmouse: true, //       overflowmethod: ‘cap’ } };"},{"elementTag":"p","innerText":"// create a timeline var timeline = new vis.timeline(container, items, options); "},{"elementTag":"h2","innerText":"release v1.0.0"},{"elementTag":"p","innerText":"you might ask why it is released now. our experience in various deployments in production shows colly provides a stable and robust platform for developing and running scrapers both locally and in multi server configuration. the feature set is complete and ready to support even complex use cases. what are those features?"},{"elementTag":"ul","innerText":"  rate limiting during scraping controlling the number of request sent to the scraped site might be crucial. we would not want to disrupt the service by overloading with too many requests. it is bad for the operators of the site and also for us, because the data we would like to collect becomes inaccessible. thus, request number must be limited. the collector provided by colly can be configured to send only a limited number of requests in parallel.   request caching to relieve the load from external services and decrease the number of outgoing requests response caching is supported.   configurable via environment variables to eliminate rebuilding of your scraper during fine-tuning, colly can read configuration options from environment variables. so you can modify its settings without a golang development environment.   proxies/proxy switchers if the address of scrapers has to be hidden proxies can be added to make requests instead of the machine running the scraping job. furthermore, to scale colly without running multiple scraper instances, proxy switchers can be used. collectors support proxy switchers which can distribute requests among multiple servers. scraping collected sites is still done on the machine running the scrapers. but the network traffic is moved to different hosts.   storage backend and storage interface during scraping a various data needs to be stored and sometimes shared. to access these objects colly provides a storage interface. you can create your own storages and use it in your scraper by implementing the interface required. by default colly saves everything into memory. additional colly backend implementations are available for redis and sqlite3.   request queue scraping pages in parallel asynchronously is a must have feature when scraping. colly maintains a request queue where urls found during scraping are collected. worker threads of your collector are taking these urls and creating requests.   goodies the package named extensions provides multiple helpers for collectors. these are common functions implemented in advance, so you don't have to bloat your scraper code with general implementations. an example extension is randomuseragent which generates a random user agent for every request. you can find the full list of goodies: https://godoc.org/github.com/gocolly/colly/extensions   debuggers debugging can be painful. colly tries to ease the pain by providing debuggers to inspect your scraper. you can simply write debug messages to the console by using logdebugger. if you prefer web interfaces, we've got you covered. colly comes with a web debugger. you can use it by initializing a webdebugger. see here how debuggers can be used: https://godoc.org/github.com/gocolly/colly/debug  "},{"elementTag":"p","innerText":"rate limiting during scraping controlling the number of request sent to the scraped site might be crucial. we would not want to disrupt the service by overloading with too many requests. it is bad for the operators of the site and also for us, because the data we would like to collect becomes inaccessible. thus, request number must be limited. the collector provided by colly can be configured to send only a limited number of requests in parallel."},{"elementTag":"p","innerText":"request caching to relieve the load from external services and decrease the number of outgoing requests response caching is supported."},{"elementTag":"p","innerText":"configurable via environment variables to eliminate rebuilding of your scraper during fine-tuning, colly can read configuration options from environment variables. so you can modify its settings without a golang development environment."},{"elementTag":"p","innerText":"proxies/proxy switchers if the address of scrapers has to be hidden proxies can be added to make requests instead of the machine running the scraping job. furthermore, to scale colly without running multiple scraper instances, proxy switchers can be used. collectors support proxy switchers which can distribute requests among multiple servers. scraping collected sites is still done on the machine running the scrapers. but the network traffic is moved to different hosts."},{"elementTag":"p","innerText":"storage backend and storage interface during scraping a various data needs to be stored and sometimes shared. to access these objects colly provides a storage interface. you can create your own storages and use it in your scraper by implementing the interface required. by default colly saves everything into memory. additional colly backend implementations are available for redis and sqlite3."},{"elementTag":"p","innerText":"request queue scraping pages in parallel asynchronously is a must have feature when scraping. colly maintains a request queue where urls found during scraping are collected. worker threads of your collector are taking these urls and creating requests."},{"elementTag":"p","innerText":"goodies the package named extensions provides multiple helpers for collectors. these are common functions implemented in advance, so you don't have to bloat your scraper code with general implementations. an example extension is randomuseragent which generates a random user agent for every request. you can find the full list of goodies: https://godoc.org/github.com/gocolly/colly/extensions"},{"elementTag":"p","innerText":"debuggers debugging can be painful. colly tries to ease the pain by providing debuggers to inspect your scraper. you can simply write debug messages to the console by using logdebugger. if you prefer web interfaces, we've got you covered. colly comes with a web debugger. you can use it by initializing a webdebugger. see here how debuggers can be used: https://godoc.org/github.com/gocolly/colly/debug"},{"elementTag":"p","innerText":"we the team behind colly believe that it has become a stable and mature scraping framework capable of supporting complex use cases. we are hoping for an even more productive future. last but not least thank you for your support and contributions."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/articles/how_to_scrape_instagram/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"how to scrape instagram profiles"},{"elementTag":"p","innerText":"scraping can be tedious work especially if the target site isn't just a standard static html page. plenty of modern sites have javascript only uis where extracting content is not always trivial. instagram is one of these websites, so i would like to show you how it is possible to write a scraper relatively fast to get images from instagram. the full working example can be found here."},{"elementTag":"h2","innerText":"information gathering"},{"elementTag":"p","innerText":"first, if we view the source code of a profile page (e.g. https://instagram.com/instagram), we can see a bunch of javascript code inside the body tag instead of static html tags. let's take a closer look at it. we can see that the first script is just a variable declaration where a huge json is assigned to a single variable (window._shareddata). this json can be easily extracted from the script tag by finding the first { character and getting the whole content after it:"},{"elementTag":"p","innerText":"note that because it is a javascript variable declaration it has a trailing semicolon what we have to cut off to get a valid json. that's why the example above ends with len(scriptcontent)-1."},{"elementTag":"p","innerText":"the formatted view of the extracted json reveals all the information we are looking for. the json contains information about a user's images and some metadata of the profile (e.g. the profile id is 25025320). there is an interesting part of the metadata called page_info:"},{"elementTag":"p","innerText":"probably, the value of end_cursor’ is the attribute of the url to get the next page when has_next_page is true."},{"elementTag":"p","innerText":"format jsons with the handy jq command line tool"},{"elementTag":"h3","innerText":"paging"},{"elementTag":"p","innerText":"the next page of the user profile is retrieved by an ajax call, so we have to use the browser's network inspector to find out what is required to fetch it. network inspector shows a long and cryptic url which has two get parameters query_id and variables:     "},{"elementTag":"p","innerText":"it seems like instagram uses a graphql api and the value of variables get parameter is an url encoded value. we can decode it with a single line of python code:"},{"elementTag":"p","innerText":"as you can see it is a json object and the value of the after attribute is the same as the value of the end_cursor and id is the id of the profile."},{"elementTag":"p","innerText":"the only unknown information in the next page url is the query_id get parameter. the html source code does not contain it, nor the cookies or response headers. after a little bit of digging it can be found in a static js file included in the main page and seems it is a constant value."},{"elementTag":"p","innerText":"the format of the response is also json but the structure is different from what we've found on the main page. this json contains the same information as the previous one, however we cannot use the same method to extract data due to structural differences."},{"elementTag":"h2","innerText":"building the scraper"},{"elementTag":"p","innerText":"the information gathering phase clearly shows that we need four building blocks to be able to fetch all images found on an instagram profile. let's do it using colly."},{"elementTag":"h3","innerText":"extract and parse json from the main page"},{"elementTag":"p","innerText":"to extract content from html we need a new collector which has a html callback to extract the json data from the script element. specifying this callback and when it must be called can be done in onhtml function of collector. the json can be easily converted to native go structure using json.unmarshal from the standard library."},{"elementTag":"h3","innerText":"create and visit next page urls"},{"elementTag":"p","innerText":"the format of the next page url is fixed, so a format string can be declared which accepts the changing id and after parameters."},{"elementTag":"h3","innerText":"parse next page jsons"},{"elementTag":"p","innerText":"this is pretty much the same as the conversion of the main page's json except these responses have some different attribute names (e.g. the image url is display_url instead of display_src)."},{"elementTag":"h3","innerText":"download and save images extracted from jsons"},{"elementTag":"p","innerText":"after requesting images from instagram using the visit function, responses can be handled in onresponse. it requires a callback as a parameter which is called after the response has arrived. to select responses which include images, we should filter based on content-type http header. if it is image, it must be saved."},{"elementTag":"h2","innerText":"epilogue"},{"elementTag":"p","innerText":"scraping js-only sites isn't always trivial, but can be handled without headless browsers and client side code execution to achieve great performance. this scraper example downloads approximately 1000 images a minute on a single thread over a regular home internet connection."},{"elementTag":"p","innerText":"it can be tweaked further to handle videos and extract meta information."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/articles/nonprofit/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"free data collection service for nonprofits"},{"elementTag":"p","innerText":"we know the importance of nonprofit and charity organizations, so we're happy to announce that we provide free data collection to them as a volunteer service. we believe in helping charities and philantropic organizations all over the world bridge the gap between their mission and their audience."},{"elementTag":"p","innerText":"our team makes it easy for your organization to create impactful research papers, infographics, statistics that you can use online or in print by collecting data for you."},{"elementTag":"p","innerText":"feel free to contact us if you are a nonprofit organization who needs any data which is publicly available on the internet."},{"elementTag":"h2","innerText":"who can apply?"},{"elementTag":"p","innerText":"all ngos, nonprofits and charities are eligible to participate in this program."},{"elementTag":"h2","innerText":"how it works?"},{"elementTag":"p","innerText":"please describe the sources you want to collect data from and the output format what you can work with. if you have a sample data set which shows the desired output, just share with us, it can help to speed up our work. output can be xml, json, csv, excel, sql database or any other custom format supported by your systems."},{"elementTag":"p","innerText":"in case of questions don't hesitate to contact us."},{"elementTag":"p","innerText":"note that we cannot provide data which is not publicly accessible or protected by captcha."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/articles/scraping_related_http_headers/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"the most important http headers for scraping"},{"elementTag":"p","innerText":"what if by simply passing parameters in the url is not enough? what can i do if i get different responses in my scraper and browser? the answer is using and understanding http headers."},{"elementTag":"p","innerText":"http headers are optional parameters of a http transaction. http requests and responses both can have different http headers. they allow the client and the server to pass additional information with the request or the response. a http header consists of its case-insensitive name followed by a colon :, then by its value (without line breaks). leading white space before the value is ignored."},{"elementTag":"p","innerText":""},{"elementTag":"p","innerText":"a detailed list of http headers can be found here."},{"elementTag":"p","innerText":"important: http headers are optional and can contain arbitrary data."},{"elementTag":"p","innerText":"always validate every http header value before using."},{"elementTag":"h2","innerText":"request headers"},{"elementTag":"p","innerText":"these header lines are sent by the client in a http protocol transaction. all lines are rfc822 format headers. the list of headers is terminated by an empty line."},{"elementTag":"h3","innerText":"cookie"},{"elementTag":"p","innerText":"http cookie (web cookie, browser cookie) is a small piece of data that a server sends to the user's request. the client may store it and send it back with the next request to the same server. typically, it's used to tell if two requests came from the same client — keeping a user logged-in, for example. it remembers stateful information for the stateless http protocol. cookies are set by the server using set-cookie. the requests send back them using cookie, so the server would know how to handle the request."},{"elementTag":"p","innerText":"sites often use cookies to implement authentication."},{"elementTag":"p","innerText":"cookies of every request in the browser can be checked using the browser's network inspector. (hotkey: f12)"},{"elementTag":"h3","innerText":"user-agent"},{"elementTag":"p","innerText":"the user-agent request header contains a characteristic string that allows the network protocol peers to identify the application type, operating system, software vendor or software version of the requesting software user agent. validating user-agent header on server side is a common operation so be sure to use valid browser's user-agent string to avoid getting blocked."},{"elementTag":"p","innerText":"example user-agents of firefox browsers:"},{"elementTag":"p","innerText":"change `user-agent` strings frequently to reduce the chance of getting blocked."},{"elementTag":"h3","innerText":"host"},{"elementTag":"p","innerText":"the host request header specifies the domain name of the server (for virtual hosting), and (optionally) the tcp port number on which the server is listening."},{"elementTag":"p","innerText":"a host header field must be sent in all http/1.1 request messages. a 400 (bad request) status code will be sent to any http/1.1 request message that lacks a host header field or contains more than one."},{"elementTag":"h3","innerText":"x-requested-with"},{"elementTag":"p","innerText":"mainly used to identify ajax requests. most javascript frameworks send this field with value of xmlhttprequest. use this header to mimic browser's ajax requests."},{"elementTag":"h3","innerText":"accept-language"},{"elementTag":"p","innerText":"the accept-language request http header advertises which languages the client is able to understand, and which locale variant is preferred. this header is a hint to be used when the server has no way of determining the language via another way, like a specific url, that is controlled by an explicit user decision."},{"elementTag":"h2","innerText":"response headers"},{"elementTag":"p","innerText":"response headers are returned by http responses from the server. they might provide further metadata of the response. their format is the same as request headers’."},{"elementTag":"h3","innerText":"content-type"},{"elementTag":"p","innerText":"this header shows the mime type of the requested resource. it can be used to determine the response body type and encoding."},{"elementTag":"h3","innerText":"content-length"},{"elementTag":"p","innerText":"the content-length response header field indicates the size of the response body, in decimal number of octets, sent to the recipient or, in the case of the head method, the size of the entity-body that would have been sent had the request been a get. (rfc2616)"},{"elementTag":"h3","innerText":"set-cookie"},{"elementTag":"p","innerText":"the set-cookie http response header is used to send cookies from the server to the client. when receiving an http request, a server can send a set-cookie header with the response. the cookie is usually sent with requests made to the same server inside a cookie http header. an expiration date or duration can be specified, after which the cookie is no longer sent. additionally, restrictions to a specific domain and path can be set, limiting where the cookie is sent."},{"elementTag":"p","innerText":"more details about cookies can be found here."},{"elementTag":"h2","innerText":"epilogue"},{"elementTag":"p","innerText":"http headers provide additional parameters to http transactions. by sending the appropriate http headers, one can access the response data in a different format. or by sending cookies back to the server, sessions can be handled. thus, making the scraper functioning."},{"elementTag":"p","innerText":"the above headers are handled automatically by browsers, that's why it is important to know how and when to use them in scraping."},{"elementTag":"p","innerText":"happy scraping!"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/articles/scraping_tips/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"web scraping tips and tricks"},{"elementTag":"p","innerText":"web scraping is extracting data from websites by getting them and selecting the relevant parts and present it in a readable or parsable format."},{"elementTag":"p","innerText":"after you decided which site or sites you want to collect data from, check if an api is available. if possible, use it.  however, if no public api is available or is not sufficient, you can turn to scraping."},{"elementTag":"p","innerText":"but how to collect data from websites and create scrapers which get the job done smoothly?"},{"elementTag":"h2","innerText":"source discovery"},{"elementTag":"p","innerText":"first, you need to specify what parts of sites contain relevant data. selectors can be used to query different elements of a html page. the two most popular selector languages are xpath and css selector. html source viewers help you come up with appropriate selectors. a simple html source viewer is available in browsers. simply right click in the page and select “view page source” from the options."},{"elementTag":"p","innerText":"the dom displayed in browser inspectors include the changes done by javascript. so it usually differs from the actual response you need to parse."},{"elementTag":"p","innerText":"always use the source code view of a page."},{"elementTag":"p","innerText":"if you prefer working form the terminal instead of browsers, wuzz is the right tool for you. it is an interactive http inspector with a terminal ui. you can learn about it more at its github page."},{"elementTag":"p","innerText":"it displays both response body and headers from a site and the contents can be searched. inspecting headers is an important part of crafting requests to get the desired data. some services require headers and cookies to return responses as seen in browsers."},{"elementTag":"p","innerText":"if you are not receiving the same result using a script from a service, try adding http headers to your requests."},{"elementTag":"h3","innerText":"js-only web pages"},{"elementTag":"p","innerText":"i am dedicating a complete subsection to js-only web pages, as there are a few misconceptions regarding them. js pages are sites which require javascript to display its contents. however, to get the data, it is not required to run phantomjs or other headless webkits. as these scripts get their data from a backend, you must discover and inspect the endpoints scripts use."},{"elementTag":"p","innerText":"discovering these endpoints can be done using the “network” pane of “developer tools” in browsers. load the page and view what requests are made to what sources. inspect the content of the responses and find the request which retrieves the data you need. as these requests are done by js, there is a good chance that responses are in json. if you have json responses there is no need to come up with selectors, you can easily parse those responses."},{"elementTag":"p","innerText":"discover internal apis using the network pane of developer tools of browsers."},{"elementTag":"h3","innerText":"selectors"},{"elementTag":"p","innerText":"coming up future proof and simple selectors can be a challenging. but there are a few best practices you can follow."},{"elementTag":"p","innerText":"look for unique identifiers in the source. for example, sidebars and content are usually separated. actual posts or content might reside under a uniquely identified element e.g. \u003cdiv id=\"content\"\u003e. these elements are tend to be persistent and contain the data in the future. furthermore, you can get rid of redundant parts of selectors by looking for unique names."},{"elementTag":"p","innerText":"use minimal unique identifiers."},{"elementTag":"p","innerText":"if the source is difficult to parse, try viewing it without javascript. sites can still show you the information, but without fancy ui elements which only gets in your way during scraping. noscript tags can contain the information in more parsable format and with less noise. or it might redirect you to a static version of the page, which is also easy to parse (e.g. google groups)."},{"elementTag":"p","innerText":"view the site without javascript."},{"elementTag":"h2","innerText":"leave as minimal footprint as possible"},{"elementTag":"p","innerText":"it is important not to interfere with users who want to use the scraped website in a regular way by viewing it in a browser. some websites adopted rate limiting or just simply ban ip addresses which are making too many requests in a short period of time. however, there are still sites which are not protected and cannot handle big loads. inaccessible services means inaccessible information, so scraping fails. to avoid hitting rate limits or disrupting services, do not make unnecessary and too many requests."},{"elementTag":"p","innerText":"waiting for a few seconds between requests is useful, because it lets services process requests without being overloaded. also, it makes your script seem more human, so it does not stand out in ordinary traffic."},{"elementTag":"p","innerText":"add delays between requests."},{"elementTag":"p","innerText":"if you are traversing multiple pages, it is a good idea to create whitelists or blacklists to differentiate between relevant and irrelevant pages. thus, the number of pages are limited only to the necessary ones."},{"elementTag":"p","innerText":"for instance, you are scraping a web shop under the domain https://shop.com/ and you are only interested in books under https://shop.com/books/. webpages of books might link to different products such as notebooks or pens. to avoid visiting those pages, a whitelist can be created which includes one pattern: https://shop.com/books/*. hence, only pages containing information on books are requested from the domain."},{"elementTag":"p","innerText":"do not visit pages you are not interested in."},{"elementTag":"p","innerText":"furthermore, it is possible that multiple pages link to the same url. there is no point in requesting it again as it has been visited previously. to eliminate duplicated requests, use a cache to store urls which has been visited already. in smaller scraping jobs simple in-memory key value stores or lists can do the trick. if you are running your scraper for so long that visited pages might change, you can invalidate the contents of your cache."},{"elementTag":"p","innerText":"cache responses."},{"elementTag":"h2","innerText":"summary"},{"elementTag":"p","innerText":"scraping is a powerful technique when you are trying to retrieve and present information in a different format. colly helps you along the way to get the data quickly and smoothly from any source."},{"elementTag":"p","innerText":"these are our tips and tricks of scraping. if you have further tips, do not hesitate to share it with us."},{"elementTag":"p","innerText":"happy scraping!"},{"elementTag":"h2","innerText":"references"},{"elementTag":"ul","innerText":" https://www.w3schools.com/xml/xpath_syntax.asp https://www.w3schools.com/cssref/css_selectors.asp https://github.com/asciimoo/wuzz "},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/contact/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"contact us"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"},{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"contact us"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/datasets/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"datasets collected by our scrapers"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"documentation"},{"elementTag":"p","innerText":"colly is a golang framework for building web scrapers. with colly you can build web scrapers of various complexity, from simple scraper to complex asynchronous website crawlers processing millions of web pages. colly provides an api for performing network requests and for handling the received content (e.g. interacting with dom tree of the html document)."},{"elementTag":"p","innerText":"below you will find some of the most common and helpful pages from our documentation."},{"elementTag":"ul","innerText":" installation guide getting started basic scraper example "},{"elementTag":"p","innerText":"also, check out godoc for api reference."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/best_practices/crawling/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"crawler configuration"},{"elementTag":"p","innerText":"colly’s default configuration is optimized for scraping smaller number of sites in one job. this setup isn’t the best if you’d like to crawl millions of sites. here are some tweaks:"},{"elementTag":"h2","innerText":"use persistent storage backend"},{"elementTag":"p","innerText":"by default colly stores cookies and visited urls in memory. you can replace the built-in in-memory storage backend with any custom backend. see more details here."},{"elementTag":"h2","innerText":"use async for long running jobs with recursive calls"},{"elementTag":"p","innerText":"by default colly blocks while the request isn’t finished, so recursively calling collector.visit from callbacks produces constantly growing stack. with collector.async = true this can be avoided. (don’t forget to use c.wait() with async.)"},{"elementTag":"h2","innerText":"disable or limit connection keep-alive"},{"elementTag":"p","innerText":"colly uses http keep-alive to enhance scraping speed. it requires open file descriptors, so max-fd limit can be easily reached with long running jobs."},{"elementTag":"p","innerText":"http keep-alive can be disabled with the following code:"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/best_practices/debugging/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"debugging"},{"elementTag":"p","innerText":"sometimes it’s enough to place some log.println() function calls to your callbacks, but sometimes it isn’t. colly has built-in abilities for collector debug. a debugger interface and different kind of debugger implementations are available."},{"elementTag":"h2","innerText":"attach debugger to a collector"},{"elementTag":"p","innerText":"attaching a basic logging debugger requires the debug (github.com/gocolly/colly/debug) package from colly’s repo."},{"elementTag":"h2","innerText":"implement a custom debugger"},{"elementTag":"p","innerText":"you can create any kind of custom debugger by implementing the debug.debugger interface. a good example is logdebugger."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/best_practices/distributed/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"distributed scraping"},{"elementTag":"p","innerText":"distributed scraping can be implemented in different ways depending on what the requirements of the scraping task are. most of the time it’s enough to scale the network communication layer which can be easily achieved using proxies and colly’s proxy switchers."},{"elementTag":"h2","innerText":"proxy switchers"},{"elementTag":"p","innerText":"using proxy switchers scraping still remains centralized while the http requests are distributed among multiple proxies. colly supports proxy switching via its’ setproxyfunc() member. any custom function can be passed to setproxyfunc() with the signature of func(*http.request) (*url.url, error)."},{"elementTag":"p","innerText":"ssh servers can be used as socks5 proxies with the -d flag."},{"elementTag":"p","innerText":"colly has a built-in proxy switcher which rotates a list of proxies on every request."},{"elementTag":"h3","innerText":"usage"},{"elementTag":"p","innerText":"implementing custom proxy switcher:"},{"elementTag":"h2","innerText":"distributed scrapers"},{"elementTag":"p","innerText":"to manage independent and distributed scrapers the best you can do is wrapping the scraper in a server. server can be any kind of service like http, tcp servers or google app engine. use custom storage to achieve centralized and persistent cookie and visited url handling."},{"elementTag":"p","innerText":"colly has built-in google app engine support. don't forget to call collector.appengine(*http.request) if you use colly from app engine standard environment."},{"elementTag":"p","innerText":"an example implementation can be found here."},{"elementTag":"h3","innerText":"distributed storage"},{"elementTag":"p","innerText":"visited url and cookie data are stored in-memory by default. this is handy for short living scraper jobs, but it can be a serious limitation when dealing with large scale or long running crawling jobs."},{"elementTag":"p","innerText":"colly has the ability to replace the default in-memory storage with any storage backend which implements colly/storage.storage interface. check out existing storages."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/best_practices/extensions/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"extensions"},{"elementTag":"p","innerText":"extensions are small helper utilities shipped with colly. list of plugins is available here."},{"elementTag":"h2","innerText":"usage"},{"elementTag":"p","innerText":"the following example enables the random user-agent switcher and the referrer setter extension and visits httpbin.org twice."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/best_practices/multi_collector/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"using multiple collectors"},{"elementTag":"p","innerText":"it is advised to use multiple collectors for one scraping jobs if the task is complex enough or has different kind of subtasks. a good example is coursera course scraper where two collectors are used - one parses the list views and handles paging and the other one collects course details."},{"elementTag":"p","innerText":"colly has some built-in methods to support the usage of multiple collectors."},{"elementTag":"p","innerText":"use collector.id in debugging to distinguish different collectors"},{"elementTag":"h2","innerText":"cloning collectors"},{"elementTag":"p","innerText":"you can use the clone() method of a collector if collectors have similar configuration. clone() duplicates a collector with identical configuration but without the attached callbacks."},{"elementTag":"h2","innerText":"passing custom data between collectors"},{"elementTag":"p","innerText":"use collector’s request() function to be able to share context with other collectors."},{"elementTag":"p","innerText":"example of sharing context:"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/best_practices/storage/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"storage backend"},{"elementTag":"p","innerText":"colly has an in-memory storage backend to store cookies and visited urls, but it can be overwritten by any custom storage backend which implements colly/storage.storage."},{"elementTag":"h2","innerText":"existing storage backends"},{"elementTag":"h3","innerText":"in-memory backend"},{"elementTag":"p","innerText":"the default backend of colly. use collector.setstorage() to override."},{"elementTag":"h3","innerText":"redis backend"},{"elementTag":"p","innerText":"see redis example for details."},{"elementTag":"h3","innerText":"sqlite3 backend"},{"elementTag":"h3","innerText":"mongodb backend"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"},{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"storage backend"},{"elementTag":"p","innerText":"colly has an in-memory storage backend to store cookies and visited urls, but it can be overwritten by any custom storage backend which implements colly/storage.storage."},{"elementTag":"h2","innerText":"existing storage backends"},{"elementTag":"h3","innerText":"in-memory backend"},{"elementTag":"p","innerText":"the default backend of colly. use collector.setstorage() to override."},{"elementTag":"h3","innerText":"redis backend"},{"elementTag":"p","innerText":"see redis example for details."},{"elementTag":"h3","innerText":"sqlite3 backend"},{"elementTag":"h3","innerText":"mongodb backend"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/basic/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"basic"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/coursera_courses/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"coursera courses"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"},{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"coursera courses"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/cryptocoinmarketcap/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"cryptocoins market capacity"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/error_handling/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"error handling"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/factbase/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"factbase"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/google_groups/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"google groups"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/hackernews_comments/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"hackernews comments"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/instagram/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"instagram images"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/login/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"login"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/max_depth/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"max depth"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/multipart/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"multipart"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/openedx_courses/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"openedx courses"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/parallel/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"parallel"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/proxy_switcher/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"proxy switcher"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/queue/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"queue"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/random_delay/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"random delay"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/rate_limit/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"rate limit"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/reddit/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"reddit"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/redis_backend/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"redis backend"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"},{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"redis backend"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/request_context/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"request context"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/scraper_server/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"scraper server"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"},{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"scraper server"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/shopify_sitemap/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"shopify sitemap"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/url_filter/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"url filter"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/examples/xkcd_store/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"xkcd store items"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/introduction/configuration/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"configuration"},{"elementTag":"p","innerText":"colly is a highly customizable scraping framework. it has sane defaults and provides plenty of options to change them."},{"elementTag":"h2","innerText":"collector configuration"},{"elementTag":"p","innerText":"full list of collector attributes can be found here. the recommended way to initialize a collector is using colly.newcollector(options...)."},{"elementTag":"p","innerText":"create a collector with default settings:"},{"elementTag":"p","innerText":"create another collector and change user-agent and url revisit options:"},{"elementTag":"p","innerText":"or"},{"elementTag":"p","innerText":"configuration can be changed at any point of a scraping job by overwriting the attributes of the collectors."},{"elementTag":"p","innerText":"a good example is a user-agent switcher which changes user-agent on every request:"},{"elementTag":"h2","innerText":"configuration via environment variables"},{"elementTag":"p","innerText":"collector’s default configuration can be changed via environment variables. this allows us to fine-tune collectors without recompile. environment parsing is the last step of the collector initialization, so every configuration change after the initialization overrides the config parsed from environment."},{"elementTag":"h3","innerText":"environment config variables"},{"elementTag":"ul","innerText":" allowed_domains (comma separated list of domains) cache_dir (string) detect_charset (y/n) disable_cookies (y/n) disallowed_domains (comma separated list of domains) ignore_robotstxt (y/n) max_body_size (int) max_depth (int - 0 means infinite) parse_http_error_response (y/n) user_agent (string) "},{"elementTag":"h2","innerText":"http configuration"},{"elementTag":"p","innerText":"colly uses golang’s default http client as networking layer. http options can be tweaked by changing the default http roundtripper."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/introduction/install/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"how to install"},{"elementTag":"p","innerText":"colly has only one prerequisite and that is golang programming language. you can install it using their installation guide."},{"elementTag":"h3","innerText":"install colly"},{"elementTag":"p","innerText":"type following command on terminal and hit enter to install colly."},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/docs/introduction/start/":[{"elementTag":"p","innerText":""},{"elementTag":"h1","innerText":"getting started"},{"elementTag":"p","innerText":"before working with colly ensure that you have the latest version. see installation guide for more details."},{"elementTag":"p","innerText":"let’s get started with some simple examples."},{"elementTag":"p","innerText":"first, you need to import colly to your codebase:"},{"elementTag":"h2","innerText":"collector"},{"elementTag":"p","innerText":"colly’s main entity is a collector object. collector manages the network communication and responsible for the execution of the attached callbacks while a collector job is running. to work with colly, you have to initialize a collector:"},{"elementTag":"h2","innerText":"callbacks"},{"elementTag":"p","innerText":"you can attach different type of callback functions to a collector to control a collecting job or retrieve information. check out the related section in the package documentation."},{"elementTag":"h3","innerText":"add callbacks to a collector"},{"elementTag":"h3","innerText":"call order of callbacks"},{"elementTag":"h4","innerText":"1. onrequest"},{"elementTag":"p","innerText":"called before a request"},{"elementTag":"h4","innerText":"2. onerror"},{"elementTag":"p","innerText":"called if error occured during the request"},{"elementTag":"h4","innerText":"3. onresponse"},{"elementTag":"p","innerText":"called after response received"},{"elementTag":"h4","innerText":"4. onhtml"},{"elementTag":"p","innerText":"called right after onresponse if the received content is html"},{"elementTag":"h4","innerText":"5. onxml"},{"elementTag":"p","innerText":"called right after onhtml if the received content is html or xml"},{"elementTag":"h4","innerText":"6. onscraped"},{"elementTag":"p","innerText":"called after onxml callbacks"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}],"https://go-colly.org/services/":[{"elementTag":"h2","innerText":"services"},{"elementTag":"h3","innerText":"custom data collection"},{"elementTag":"p","innerText":"you provide the requirements and our scraping gurus deliver the desired data to you."},{"elementTag":"h3","innerText":"                                  free             "},{"elementTag":"p","innerText":"free data collection service to support ngos and non-profits.we to get data for you."},{"elementTag":"h4","innerText":"only for ngos and non-profits"},{"elementTag":"h3","innerText":"                                  instant             "},{"elementTag":"p","innerText":"gather data from up to 10 websites within a week.fast and reliable."},{"elementTag":"h4","innerText":"$200 per site"},{"elementTag":"h3","innerText":"                                  advanced             "},{"elementTag":"p","innerText":"unlimited number of websites.let us handle your scrapers and enjoy the data."},{"elementTag":"h4","innerText":"from $1,500"},{"elementTag":"h4","innerText":"© 2018 colly team"},{"elementTag":"p","innerText":"page sourcesite map"}]}